---
title: 高并发分布式项目亿级流量访问解决方案
date: 2020-05-13 00:58:22
tags: 限流
categories: 高并发
---

# 1 亿级流量系统如何架构设计应对流量问题
流量漏斗：HTTP请求 -> DNS -> 网关（Nginx/F5） -> 应用服务（无状态-中心化） -> 缓存（Redis） -> 数据库（分库分表）

# 2 突发流量过高的情况下如何进行系统保护
503/504服务不可用(Nginx) 
计划型的设计，主要应对已知风险：

- 动静分离：前后端数据分离，将静态资源（图片/视频/JS/CSS/font）存放到CDN
- 扩容：需要支持扩容，前提是无状态，即服务节点要支持柔性扩展
- 缓存：引入缓存层将数据从CPU-I/O转变成内存方式

当流量激增，我们所有计划型的设计就无效了。
以下是两个应急设计：
- 降级：可以通过一些开关或服务设置的一些阈值进行服务的本地存根调用，降级熔断核心目的是为了不让系统出现服务雪崩。非核心业务的CPU、内存等资源倾斜给主链路业务。
- 限流：确保系统最后的生存底线

# 3 系统保护的终极方案：限流设计

提示服务正忙，无情把用户拒之门外
购买前先进行验证码验证，就是一种业务限流的方式
明确的系统限流设计？

# 4 分布式限流的维度、算法、解决方案分析
## 4.1 分布式限流的维度
有两个核心的维度
- 时间：也就是我们常说的时间窗口，比如每分钟、每秒
- 资源：时间窗口下可以使用的资源，比如最大访问量、最高可用连接数

常用的几种限流规则：
- QPS和连接数：可以基于某个IP或某个服务器进行限制
- 传输速率：比如百度网盘
- 黑白名单：只有开放了白名单的IP和用户才能进行数据API数据的调用，对无效且不友好的访问IP进行黑名单限制

## 4.2 分布式限流的环境
分布式一定有很多服务节点，有这么多服务节点，来进行限流的控制和校验一定是中心化
- 网关层限流：所有流量的入口，Nginx或者其他的负载均衡HAProxy/Lvs/SLB/F5
- 中间件限流：比如Redis来做中间件限流控制

## 4.3 分布式限流的算法分析
### 4.3.1 令牌桶算法
如果桶里没有令牌，这时候系统服务无法调用？
- 阻塞式，等
- 非阻塞，返回异常或者相应信息

### 4.3.2 漏桶算法
- 恒定速率流出
- 无法应对突发大流量

## 4.4 限流的实现方案
### 4.4.1 客户端限流
在单机服务上进行流量控制，可以使用Google的Guava

### 4.4.2 网关层限流
网关层作为系统流量的入口可以进行所有用户的访问控制
Nginx，SpringCloud Gateway/Zuul，F5

### 4.4.3 中间件限流
流量已经从网关流入业务层，每个业务都通过Nginx来进行访问，Redis就可以进行中间件限流使用

### 4.4.4 限流组件
比如SpringCloud Alibaba sentinel流量防卫兵

# 5 通过Google Guava实现客户端方式限流

RateLimiter令牌桶，用于单机服务

```java
public class RateLimiterTest {
    private static final SimpleDateFormat FORMATTER = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); 
    private static final int THREAD_COUNT = 25;
    
    @Test
    public void testRateLimiter1() {
        RateLimiter rateLimiter = RateLimiter.create(5);
        
        Thread[] ts = new Thread[THREAD_COUNT];
        for (int i = 0; i < THREAD_COUNT; i++) {
            ts[i] = new Thread(new RateLimiterThread(rateLimiter), "RateLimiterThread-" + i);
        }
        
        for (int i = 0; i < THREAD_COUNT; i++) {
            ts[i].start();
        }
        
        for (;;);
    }
    
    public class RateLimiterThread implements Runnable {
        private RateLimiter rateLimiter;
        public RateLimiterThread(RateLimiter rateLimiter) {
            this.rateLimiter = rateLimiter;
        }
        
        @Override
        public void run() {
            rateLimiter.acquire(1);
            System.out.println(Thread.currentThread().getName() + "获取到了令牌，时间 = " + FORMATTER.format(new Date()));
        }
    }
}
```

# 6 使用Nginx实现网关层限流和数据响应
## 6.1 来源IP限流
```shell
# 根据IP地址限制速率
# 1.$binary_remote_addr 获取IP地址的内存数据占用
# 2.iplimit 自定义的内存区域 10m指内存大小
# 3.rate=1r/s 100r/m
limit_req_zone $binary_remote_addr zone=iplimit:10m rate=1r/s;

server{
	server_name limit.icodingedu.com;
	location /access-limit/ {
		proxy_pass http://locahost:8088/;
		limit_req zone=iplimit burst=2 nodelay;
	}
}
```

## 6.2 服务器级别做限流
```shell
limit_req_zone $binary_remote_addr zone=iplimit:10m rate=100r/s;
# 增加服务器限流的规则
limit_req_zone $server_name zone=serverlimit:10m rate=1r/s;

server{
	server_name limit.icodingedu.com;
	location /access-limit/ {
		proxy_pass http://locahost:8088/;
		limit_req zone=iplimit burst=2 nodelay;
		limit_req zone=serverlimit burst=1 nodelay;
	}
}
```

## 6.3 基于连接数的限制
```shell
limit_req_zone $binary_remote_addr zone=iplimit:10m rate=100r/s;
limit_req_zone $server_name zone=serverlimit:10m rate=100r/s;
# 基于IP的连接数限制
limit_conn_zone $binary_remote_addr zone=perip:10m;
# 基于服务器的连接数限制
limit_conn_zone $server_name zone=perserver:10m;

server{
	server_name limit.icodingedu.com;
	location /access-limit/ {
		proxy_pass http://locahost:8088/;
		limit_req zone=iplimit burst=2 nodelay;
		limit_req zone=serverlimit burst=1 nodelay;
		# 应用连接限制并定义连接数
		limit_conn perserver 100;
		limit_conn perip 1;
		# 对限制的请求设置状态码
		limit_req_status 429;
		limit_conn_status 429;
	}
	# 自定义返回信息
	error_page 429 /429;
	lcoation /429 {
		default_type application/json;
		add_header Content-Type 'text/html;charset=utf-8';
		return 200 '{"code":"429","msg":"访问高峰，请稍后再试......"}';
	}
	# 下载限速
	location /download/ {
		limit_rate_after 100m;
		limit_rate 256k;
	}
}
```

# 7 使用Redis+Lua脚本实现中间件限流
Nginx获得的IP才是第一手，在网关层进行IP限制才是最合适的。应用层之间也会进行流量激增或服务的横向调用，服务之间也要引入流量控制，这个地方不能用Nginx。
Redis天生可以用来做限流
Redis + Lua：解决频繁调用的原子性。方式灵活，可以使用在业务层的任何地方

```shell
eval script numkeys key [key ...] arg [arg ...]
```

```java
@Configuration
publiv class RedisConfiger {
	@Bean
	public RedisTemplate<String, String> redisTemplate(RedisConnectionFactory redisConnectionFactory) {
		return new StringRedisTemplate(redisConnectionFactory);
	}
	@Bean
	public DefaultRedisScript loadRedisScript() {
		DefaultRedisScript redisScript = new DefaultReidsScript();
		// lua脚本要放在resource下
		redisScript.setLocation(new ClassPathResource("ratelimiter.lua"));
		redisScript.setResultType(java.lang.Boolean.class);
		return redisScript;
	}
}
```

```java
@Slf4j
public class AccessLimiterService {
	@Autowired
	private StringRedisRemplate stringRedisTemplate;
	@Autowired
	private RedisScript<RateLimiter> rateLimitLua;
	
	// key:限流业务的唯一id，limit:一秒内可以访问多少次
	public void limitAccess(String key, Integer limit) {
		boolean acquire = stringRedisTemplate.execute(
			rateLimitLua, // lua脚本本身
			Lists.newArrayList(key), // 限流肯定是对某个API
			limit.toString()
		);
		if (!acquire) {
			log.error("your access is block key is {}", key);
			throw new RuntimeException("YOUR ACCESS IS BLOCKED");
		}
	}
}
```

```lua
-- 获取业务id
local busKey = KEYS[1]
-- 限流大小
local limit = tonumber(ARGV[1])
-- 获取当前流量大小
local count = tonumber(redis.call("get", busKey) or "0")
-- 判断是否超流量限制
if count + 1 > limit then
	return false
else
	redis.call("incrby", busKey, 1)
	redis.call("expire", busKey, 1)
	return true
end
```

# 8 SpringCloud Alibaba限流组件Sentinel实现微服务模块间限流
```xml
<dependency>
	<groupId>org.springframework.cloud</groupId>
	<artifactId>spirng-cloud-starter-alibaba-sentinel</artifactId>
	<version>0.9.0.RELEASE</version>
</dependency>
```
需要一个sentinel的控制台：sentinel-dashboard-1.7.2.jar
pom.xml加spring.cloud.sentinel.transport.dashboard:localhost:8080